set -euxo pipefail

# Build gcloud dataproc job submission command
gcloud_command="gcloud dataproc jobs submit spark"

# Dataproc environment parameters
gcloud_command+=" --cluster=${CLUSTER_NAME:-chronon-cluster}"  # Dataproc cluster name (required)
gcloud_command+=" --region=${REGION:-us-central1}"        # Dataproc cluster region (required)

# Set offline and online jars
gcloud_command+=" --jars=${CHRONON_DRIVER_JAR:-}"
if [ -n "${CHRONON_ONLINE_JAR:-}" ]; then
    gcloud_commmand+=",${CHRONON_ONLINE_JAR}"
fi

gcloud_command+=" --properties="  # Set Dataproc cluster properties (key=value pairs)

# Spark parameters
gcloud_command+="spark.sql.shuffle.partitions=${PARALLELISM:-4000},"
gcloud_command+="spark.dynamicAllocation.maxExecutors=${MAX_EXECUTORS:-1000},"
gcloud_command+="spark.default.parallelism=${PARALLELISM:-4000},"
gcloud_command+="spark.executor.cores=${EXECUTOR_CORES:-1},"
gcloud_command+="spark.app.name=${APP_NAME:-chronon-app-name},"

# Chronon specific parameters
gcloud_command+="spark.chronon.partition.column=${PARTITION_COLUMN:-ds},"
gcloud_command+="spark.chronon.partition.format=${PARTITION_FORMAT:-yyyy-MM-dd},"
gcloud_command+="spark.chronon.backfill.validation.enabled=${ENABLE_VALIDATION:-false},"
gcloud_command+="spark.chronon.outputParallelismOverride=${OUTPUT_PARALLELISM:--1},"
gcloud_command+="spark.chronon.rowCountPerPartition=${ROW_COUNT_PER_PARTITION:--1},"
gcloud_command+="spark.chronon.sql.format=bigquery,"

# BigQuery connector required attributes
gcloud_command+="materializationDataset=${MATERIALIZATION_DATASET:-chronon_tmp},"
gcloud_command+="viewsEnabled=${VIEWS_ENABLED:-true}"

# Arguments generated by the run.py script
gcloud_command+=" $@"

# Submit the job
$gcloud_command 2>&1
